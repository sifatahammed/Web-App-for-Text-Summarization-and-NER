{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXRFmFoqvz7s"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn nest-asyncio pyngrok transformers pdfminer.six PyPDF2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtnTlifGQIHh",
        "outputId": "2247cf4f-bd58-4cab-c520-6bccdf12a1b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install safetensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ILK6ITAA-cn"
      },
      "outputs": [],
      "source": [
        "!pip install python-multipart\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP8heIKr9vxF",
        "outputId": "a37e5db0-f17c-41d3-9c68-4a9627728d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, UploadFile, File, HTTPException\n",
        "from PyPDF2 import PdfReader\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from transformers import (\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    TFAutoModelForSeq2SeqLM,\n",
        "    AutoModelForTokenClassification,\n",
        ")\n",
        "import uvicorn\n",
        "import nest_asyncio\n",
        "from pdfminer.high_level import extract_text\n",
        "import csv\n",
        "from pyngrok import ngrok\n",
        "\n"
      ],
      "metadata": {
        "id": "nFpYhRq4VsG0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN-Y6kjUDrpy",
        "outputId": "32097b39-4e64-400c-94c4-c3a8b6ba7905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Public URL: https://6e7b-35-233-177-138.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken ***********\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print(f\"Public URL: {ngrok_tunnel.public_url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3KBm07Lv6os"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "# Allow CORS for frontend communication\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Paths to saved models and tokenizer directories\n",
        "summarization_model_path = \"/content/drive/MyDrive/text_summarization\"\n",
        "summarization_tokenizer_path = \"/content/drive/MyDrive/text_summariztion_tf\"\n",
        "ner_model_path = \"/content/drive/MyDrive/NER/Model/NER\"\n",
        "ner_tokenizer_path = \"/content/drive/MyDrive/NER/Model/NER_tf\"\n",
        "\n",
        "# Load models and tokenizers\n",
        "# Text Summarization (TensorFlow)\n",
        "summarization_tokenizer = AutoTokenizer.from_pretrained(summarization_tokenizer_path)\n",
        "summarization_model = TFAutoModelForSeq2SeqLM.from_pretrained(summarization_model_path)\n",
        "summarization_pipeline = pipeline(\n",
        "    \"summarization\",\n",
        "    model=summarization_model,\n",
        "    tokenizer=summarization_tokenizer,\n",
        "    framework=\"tf\",\n",
        ")\n",
        "\n",
        "# Named Entity Recognition (PyTorch)\n",
        "ner_tokenizer = AutoTokenizer.from_pretrained(ner_tokenizer_path)\n",
        "ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_path)\n",
        "ner_pipeline = pipeline(\n",
        "    \"ner\",\n",
        "    model=ner_model,\n",
        "    tokenizer=ner_tokenizer,\n",
        "    aggregation_strategy=\"simple\"\n",
        ")\n",
        "\n",
        "# API Models\n",
        "class TextRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "\n",
        "# Root endpoint\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"Backend is running!\"}\n",
        "\n",
        "# Summarization endpoint\n",
        "@app.post(\"/summarization\")\n",
        "def summarize_text(request: TextRequest):\n",
        "    try:\n",
        "        summary = summarization_pipeline(request.text, max_length=130, min_length=70, do_sample=False)\n",
        "        return {\"summary\": summary[0][\"summary_text\"]}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error during summarization: {str(e)}\")\n",
        "\n",
        "# NER endpoint\n",
        "@app.post(\"/ner\")\n",
        "def ner_text(request: TextRequest):\n",
        "    try:\n",
        "        # Tokenize input\n",
        "        inputs = ner_tokenizer(\n",
        "            request.text,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "        # Perform inference\n",
        "        outputs = ner_model(**inputs)\n",
        "        predictions = outputs.logits.argmax(-1)\n",
        "\n",
        "        # Convert predictions to labels\n",
        "        tokens = ner_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze())\n",
        "        label_map = {\n",
        "                    0: \"O\",         # Outside of an entity\n",
        "                    1: \"B-PER\",     # Beginning of a person's name\n",
        "                    2: \"I-PER\",     # Inside a person's name\n",
        "                    3: \"B-LOC\",     # Beginning of a location\n",
        "                    4: \"I-LOC\",     # Inside a location\n",
        "                    5: \"B-ORG\",     # Beginning of an organization\n",
        "                    6: \"I-ORG\",     # Inside an organization\n",
        "                    7: \"B-MISC\",    # Beginning of a miscellaneous entity\n",
        "                    8: \"I-MISC\",    # Inside a miscellaneous entity\n",
        "                }\n",
        "        predicted_labels = [label_map[label] for label in predictions.squeeze().tolist()]\n",
        "        tokens = [token.replace(\"‚ñÅ\", \"\") for token in tokens]\n",
        "\n",
        "        # Filter out \"O\" labels\n",
        "        filtered_results = [\n",
        "            (token, label)\n",
        "            for token, label in zip(tokens, predicted_labels)\n",
        "            if label != \"O\"\n",
        "        ]\n",
        "        for token, label in filtered_results:\n",
        "          print(f\"{token}: {label}\")\n",
        "        return {\"entities\": filtered_results}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error during NER: {str(e)}\")\n",
        "\n",
        "\n",
        "# File upload endpoint (supports .txt and .pdf)\n",
        "@app.post(\"/upload\")\n",
        "async def upload_file(file: UploadFile = File(...)):\n",
        "    try:\n",
        "        if file.filename.endswith(\".txt\"):\n",
        "            content = await file.read()\n",
        "            text = content.decode(\"utf-8\")\n",
        "        elif file.filename.endswith(\".pdf\"):\n",
        "            pdf_reader = PdfReader(file.file)\n",
        "            text = \"\"\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        else:\n",
        "            raise HTTPException(status_code=400, detail=\"Unsupported file format\")\n",
        "        return {\"text\": text}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error processing file: {str(e)}\")\n",
        "\n",
        "# Health check endpoint for testing the backend\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"Healthy!\"}\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}